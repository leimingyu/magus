{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys # error msg, add the modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"../pycode/\")\n",
    "from magus_util import read_nvprof_trace, parse_nvprof_trace, getruntime, sort_dict_by_val, genNNFeat\n",
    "from magus_contention import *\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### obtain trace files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binopt_trace.csv', 'sortingNetworks_trace.csv', 'transpose_trace.csv', 'SobolQRNG_trace.csv', 'reduction_trace.csv', 'matrixMul_trace.csv', 'MC_SingleAsianOptionP_trace.csv', 'mergeSort_trace.csv', 'interval_trace.csv', 'quasirandomGenerator_trace.csv', 'convolutionFFT2D_trace.csv', 'radixSortThrust_trace.csv', 'scan_trace.csv']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# 13 cuda apps\n",
    "#\n",
    "traceFolder = \"./test_files\"\n",
    "appTraces = os.listdir(traceFolder)\n",
    "print appTraces\n",
    "print len(appTraces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FDTD3d_trace.csv', 'convolutionSeparable_trace.csv', 'dct8x8_trace.csv', 'nvgraph_Pagerank_trace.csv', 'histogram_trace.csv', 'batchCUBLAS_trace.csv', 'simpleCUFFT_callback_trace.csv', 'conjugateGradient_trace.csv', 'boxFilterNPP_trace.csv', 'BlackScholes_trace.csv']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# additional 10 apps \n",
    "#\n",
    "traceFolder_extra = \"./test_apps_trace\"\n",
    "appTraces_extra = os.listdir(traceFolder_extra)\n",
    "print appTraces_extra\n",
    "print len(appTraces_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate feature matrix based on traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 13 cuda apps \n",
      "\n",
      "binopt\n",
      "sortingNetworks\n",
      "transpose\n",
      "SobolQRNG\n",
      "reduction\n",
      "matrixMul\n",
      "MC_SingleAsianOptionP\n",
      "mergeSort\n",
      "interval\n",
      "quasirandomGenerator\n",
      "convolutionFFT2D\n",
      "radixSortThrust\n",
      "scan\n",
      "\n",
      " extra 10 cuda apps \n",
      "\n",
      "FDTD3d\n",
      "convolutionSeparable\n",
      "dct8x8\n",
      "nvgraph_Pagerank\n",
      "histogram\n",
      "batchCUBLAS\n",
      "simpleCUFFT_callback\n",
      "conjugateGradient\n",
      "boxFilterNPP\n",
      "BlackScholes\n"
     ]
    }
   ],
   "source": [
    "app_feat_array = None\n",
    "app_rowid_dict = {}\n",
    "appTraceDict = {}\n",
    "\n",
    "#\n",
    "# 13 cuda apps\n",
    "#\n",
    "print('\\n 13 cuda apps \\n')\n",
    "for i, eachAppTrace in enumerate(appTraces):\n",
    "    appName = eachAppTrace[:-10]\n",
    "    print appName\n",
    "    file_csv = traceFolder + '/' + eachAppTrace \n",
    "    #print file_csv\n",
    "    df_trace = read_nvprof_trace(file_csv)\n",
    "    appTraceList = parse_nvprof_trace(df_trace)\n",
    "    #print appTraceList\n",
    "    appTraceDict[appName] = appTraceList\n",
    "    \n",
    "    if i == 0:\n",
    "        app_feat_array = genNNFeat(appTraceList)\n",
    "    else:\n",
    "        app_feat_array = np.vstack((app_feat_array, genNNFeat(appTraceList)))\n",
    "        \n",
    "    #print i\n",
    "    app_rowid_dict[appName] = i\n",
    "\n",
    "#\n",
    "# additional 10 apps \n",
    "#   \n",
    "print('\\n extra 10 cuda apps \\n')\n",
    "for i, eachAppTrace in enumerate(appTraces_extra):\n",
    "    appName = eachAppTrace[:-10]\n",
    "    print appName\n",
    "    \n",
    "    file_csv = traceFolder_extra + '/' + eachAppTrace \n",
    "    #print file_csv\n",
    "    \n",
    "    df_trace = read_nvprof_trace(file_csv)\n",
    "    appTraceList = parse_nvprof_trace(df_trace)\n",
    "    #print appTraceList\n",
    "    \n",
    "    appTraceDict[appName] = appTraceList\n",
    "    \n",
    "    app_feat_array = np.vstack((app_feat_array, genNNFeat(appTraceList)))\n",
    "     \n",
    "    #print i + 13\n",
    "    app_rowid_dict[appName] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h2d', 566.138617, 566.141849, 0.0, 0.0, 0.0, 0.0],\n",
       " ['kern', 566.150169, 575.913912, 1024.0, 128.0, 32.0, 516.0],\n",
       " ['d2h', 575.915512, 575.917496, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appTrace = appTraceDict['binopt']\n",
    "appTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h2d', 12137.205, 12703.343918, 0.0, 0.0, 0.0, 0.0],\n",
       " ['h2d', 12703.43, 13269.573654, 0.0, 0.0, 0.0, 0.0],\n",
       " ['h2d', 13269.586000000001, 13269.587184000002, 0.0, 0.0, 0.0, 0.0],\n",
       " ['kern', 13269.617999999999, 13272.026054999998, 288.0, 512.0, 40.0, 3750.0],\n",
       " ['kern', 13272.027, 13274.409998, 288.0, 512.0, 40.0, 3750.0],\n",
       " ['kern', 13274.412, 13276.803031, 288.0, 512.0, 40.0, 3750.0],\n",
       " ['kern', 13276.805, 13279.185374, 288.0, 512.0, 40.0, 3750.0],\n",
       " ['kern', 13279.187, 13281.584623, 288.0, 512.0, 40.0, 3750.0],\n",
       " ['d2h', 13281.599999999999, 13823.284344999998, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appTrace = appTraceDict['FDTD3d']\n",
    "appTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 45)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_feat_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BlackScholes': 9,\n",
       " 'FDTD3d': 0,\n",
       " 'MC_SingleAsianOptionP': 6,\n",
       " 'SobolQRNG': 3,\n",
       " 'batchCUBLAS': 5,\n",
       " 'binopt': 0,\n",
       " 'boxFilterNPP': 8,\n",
       " 'conjugateGradient': 7,\n",
       " 'convolutionFFT2D': 10,\n",
       " 'convolutionSeparable': 1,\n",
       " 'dct8x8': 2,\n",
       " 'histogram': 4,\n",
       " 'interval': 8,\n",
       " 'matrixMul': 5,\n",
       " 'mergeSort': 7,\n",
       " 'nvgraph_Pagerank': 3,\n",
       " 'quasirandomGenerator': 9,\n",
       " 'radixSortThrust': 11,\n",
       " 'reduction': 4,\n",
       " 'scan': 12,\n",
       " 'simpleCUFFT_callback': 6,\n",
       " 'sortingNetworks': 1,\n",
       " 'transpose': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row index in app_feat_array[]\n",
    "app_rowid_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the ground truth files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'mergeSort', u'reduction', 1]\n",
      "[u'mergeSort', u'SobolQRNG', 0]\n",
      "[u'mergeSort', u'scan', 0]\n",
      "[u'mergeSort', u'matrixMul', 0]\n",
      "[u'mergeSort', u'convfft2d', 1]\n",
      "[u'mergeSort', u'quasirandomGenerator', 1]\n",
      "[u'mergeSort', u'binopt', 1]\n",
      "[u'mergeSort', u'interval', 0]\n",
      "[u'mergeSort', u'MCSingleAsianOptionP', 0]\n",
      "[u'mergeSort', u'transpose', 0]\n",
      "[u'mergeSort', u'radixSortThrust', 1]\n",
      "[u'mergeSort', u'sortingNetworks', 1]\n",
      "[u'radixSortThrust', u'scan', 0]\n",
      "[u'radixSortThrust', u'reduction', 0]\n",
      "[u'radixSortThrust', u'mergeSort', 1]\n",
      "[u'radixSortThrust', u'interval', 1]\n",
      "[u'radixSortThrust', u'matrixMul', 1]\n",
      "[u'radixSortThrust', u'SobolQRNG', 1]\n",
      "[u'radixSortThrust', u'transpose', 1]\n",
      "[u'radixSortThrust', u'sortingNetworks', 1]\n",
      "[u'radixSortThrust', u'binopt', 1]\n",
      "[u'radixSortThrust', u'convfft2d', 1]\n",
      "[u'radixSortThrust', u'quasirandomGenerator', 1]\n",
      "[u'radixSortThrust', u'MCSingleAsianOptionP', 0]\n",
      "[u'scan', u'MCSingleAsianOptionP', 0]\n",
      "[u'scan', u'convfft2d', 1]\n",
      "[u'scan', u'interval', 1]\n",
      "[u'scan', u'reduction', 0]\n",
      "[u'scan', u'radixSortThrust', 0]\n",
      "[u'scan', u'transpose', 0]\n",
      "[u'scan', u'quasirandomGenerator', 1]\n",
      "[u'scan', u'SobolQRNG', 1]\n",
      "[u'scan', u'mergeSort', 0]\n",
      "[u'scan', u'matrixMul', 0]\n",
      "[u'scan', u'binopt', 1]\n",
      "[u'scan', u'sortingNetworks', 1]\n",
      "[u'SobolQRNG', u'mergeSort', 1]\n",
      "[u'SobolQRNG', u'quasirandomGenerator', 0]\n",
      "[u'SobolQRNG', u'matrixMul', 0]\n",
      "[u'SobolQRNG', u'reduction', 1]\n",
      "[u'SobolQRNG', u'interval', 0]\n",
      "[u'SobolQRNG', u'scan', 1]\n",
      "[u'SobolQRNG', u'sortingNetworks', 0]\n",
      "[u'SobolQRNG', u'radixSortThrust', 1]\n",
      "[u'SobolQRNG', u'MCSingleAsianOptionP', 0]\n",
      "[u'SobolQRNG', u'convfft2d', 0]\n",
      "[u'SobolQRNG', u'binopt', 0]\n",
      "[u'SobolQRNG', u'transpose', 0]\n",
      "[u'transpose', u'convfft2d', 0]\n",
      "[u'transpose', u'SobolQRNG', 0]\n",
      "[u'transpose', u'reduction', 1]\n",
      "[u'transpose', u'MCSingleAsianOptionP', 0]\n",
      "[u'transpose', u'interval', 0]\n",
      "[u'transpose', u'binopt', 0]\n",
      "[u'transpose', u'mergeSort', 0]\n",
      "[u'transpose', u'scan', 0]\n",
      "[u'transpose', u'matrixMul', 0]\n",
      "[u'transpose', u'sortingNetworks', 0]\n",
      "[u'transpose', u'radixSortThrust', 1]\n",
      "[u'transpose', u'quasirandomGenerator', 0]\n",
      "[u'interval', u'radixSortThrust', 1]\n",
      "[u'interval', u'convfft2d', 1]\n",
      "[u'interval', u'reduction', 1]\n",
      "[u'interval', u'MCSingleAsianOptionP', 0]\n",
      "[u'interval', u'binopt', 1]\n",
      "[u'interval', u'mergeSort', 0]\n",
      "[u'interval', u'SobolQRNG', 0]\n",
      "[u'interval', u'scan', 1]\n",
      "[u'interval', u'matrixMul', 0]\n",
      "[u'interval', u'quasirandomGenerator', 1]\n",
      "[u'interval', u'sortingNetworks', 1]\n",
      "[u'interval', u'transpose', 0]\n",
      "[u'binopt', u'reduction', 1]\n",
      "[u'binopt', u'radixSortThrust', 1]\n",
      "[u'binopt', u'sortingNetworks', 1]\n",
      "[u'binopt', u'MCSingleAsianOptionP', 0]\n",
      "[u'binopt', u'transpose', 0]\n",
      "[u'binopt', u'interval', 1]\n",
      "[u'binopt', u'convfft2d', 1]\n",
      "[u'binopt', u'scan', 1]\n",
      "[u'binopt', u'matrixMul', 0]\n",
      "[u'binopt', u'quasirandomGenerator', 1]\n",
      "[u'binopt', u'SobolQRNG', 0]\n",
      "[u'binopt', u'mergeSort', 1]\n",
      "[u'matrixMul', u'binopt', 0]\n",
      "[u'matrixMul', u'SobolQRNG', 0]\n",
      "[u'matrixMul', u'radixSortThrust', 1]\n",
      "[u'matrixMul', u'transpose', 0]\n",
      "[u'matrixMul', u'quasirandomGenerator', 0]\n",
      "[u'matrixMul', u'interval', 0]\n",
      "[u'matrixMul', u'sortingNetworks', 0]\n",
      "[u'matrixMul', u'scan', 0]\n",
      "[u'matrixMul', u'reduction', 0]\n",
      "[u'matrixMul', u'MCSingleAsianOptionP', 0]\n",
      "[u'matrixMul', u'convfft2d', 0]\n",
      "[u'matrixMul', u'mergeSort', 0]\n",
      "[u'quasirandomGenerator', u'mergeSort', 1]\n",
      "[u'quasirandomGenerator', u'reduction', 1]\n",
      "[u'quasirandomGenerator', u'sortingNetworks', 1]\n",
      "[u'quasirandomGenerator', u'interval', 1]\n",
      "[u'quasirandomGenerator', u'binopt', 1]\n",
      "[u'quasirandomGenerator', u'scan', 1]\n",
      "[u'quasirandomGenerator', u'transpose', 0]\n",
      "[u'quasirandomGenerator', u'SobolQRNG', 0]\n",
      "[u'quasirandomGenerator', u'convfft2d', 1]\n",
      "[u'quasirandomGenerator', u'matrixMul', 0]\n",
      "[u'quasirandomGenerator', u'radixSortThrust', 1]\n",
      "[u'quasirandomGenerator', u'MCSingleAsianOptionP', 0]\n",
      "[u'reduction', u'scan', 0]\n",
      "[u'reduction', u'matrixMul', 1]\n",
      "[u'reduction', u'binopt', 1]\n",
      "[u'reduction', u'interval', 1]\n",
      "[u'reduction', u'mergeSort', 1]\n",
      "[u'reduction', u'quasirandomGenerator', 1]\n",
      "[u'reduction', u'convfft2d', 1]\n",
      "[u'reduction', u'radixSortThrust', 0]\n",
      "[u'reduction', u'SobolQRNG', 1]\n",
      "[u'reduction', u'sortingNetworks', 1]\n",
      "[u'reduction', u'MCSingleAsianOptionP', 0]\n",
      "[u'reduction', u'transpose', 1]\n",
      "[u'convfft2d', u'interval', 1]\n",
      "[u'convfft2d', u'MCSingleAsianOptionP', 0]\n",
      "[u'convfft2d', u'reduction', 1]\n",
      "[u'convfft2d', u'quasirandomGenerator', 1]\n",
      "[u'convfft2d', u'mergeSort', 1]\n",
      "[u'convfft2d', u'radixSortThrust', 1]\n",
      "[u'convfft2d', u'binopt', 1]\n",
      "[u'convfft2d', u'matrixMul', 0]\n",
      "[u'convfft2d', u'sortingNetworks', 1]\n",
      "[u'convfft2d', u'transpose', 0]\n",
      "[u'convfft2d', u'SobolQRNG', 0]\n",
      "[u'convfft2d', u'scan', 1]\n",
      "[u'MCSingleAsianOptionP', u'transpose', 0]\n",
      "[u'MCSingleAsianOptionP', u'convfft2d', 0]\n",
      "[u'MCSingleAsianOptionP', u'radixSortThrust', 1]\n",
      "[u'MCSingleAsianOptionP', u'binopt', 0]\n",
      "[u'MCSingleAsianOptionP', u'SobolQRNG', 0]\n",
      "[u'MCSingleAsianOptionP', u'scan', 0]\n",
      "[u'MCSingleAsianOptionP', u'quasirandomGenerator', 0]\n",
      "[u'MCSingleAsianOptionP', u'mergeSort', 0]\n",
      "[u'MCSingleAsianOptionP', u'sortingNetworks', 0]\n",
      "[u'MCSingleAsianOptionP', u'matrixMul', 0]\n",
      "[u'MCSingleAsianOptionP', u'interval', 0]\n",
      "[u'MCSingleAsianOptionP', u'reduction', 0]\n",
      "[u'sortingNetworks', u'interval', 1]\n",
      "[u'sortingNetworks', u'radixSortThrust', 1]\n",
      "[u'sortingNetworks', u'mergeSort', 1]\n",
      "[u'sortingNetworks', u'scan', 1]\n",
      "[u'sortingNetworks', u'reduction', 1]\n",
      "[u'sortingNetworks', u'SobolQRNG', 0]\n",
      "[u'sortingNetworks', u'MCSingleAsianOptionP', 0]\n",
      "[u'sortingNetworks', u'quasirandomGenerator', 1]\n",
      "[u'sortingNetworks', u'convfft2d', 1]\n",
      "[u'sortingNetworks', u'binopt', 1]\n",
      "[u'sortingNetworks', u'transpose', 0]\n",
      "[u'sortingNetworks', u'matrixMul', 0]\n",
      "[u'FDTD3d', u'histogram', 1]\n",
      "[u'FDTD3d', u'batchCUBLAS', 1]\n",
      "[u'FDTD3d', u'boxFilterNPP', 1]\n",
      "[u'FDTD3d', u'dct8x8', 1]\n",
      "[u'FDTD3d', u'simpleCUFFTcallback', 1]\n",
      "[u'FDTD3d', u'convolutionSeparable', 1]\n",
      "[u'FDTD3d', u'nvgraphPagerank', 1]\n",
      "[u'FDTD3d', u'conjugateGradient', 1]\n",
      "[u'FDTD3d', u'BlackScholes', 1]\n",
      "[u'BlackScholes', u'FDTD3d', 1]\n",
      "[u'BlackScholes', u'histogram', 1]\n",
      "[u'BlackScholes', u'nvgraphPagerank', 1]\n",
      "[u'BlackScholes', u'boxFilterNPP', 1]\n",
      "[u'BlackScholes', u'convolutionSeparable', 1]\n",
      "[u'BlackScholes', u'dct8x8', 1]\n",
      "[u'BlackScholes', u'simpleCUFFTcallback', 1]\n",
      "[u'BlackScholes', u'conjugateGradient', 1]\n",
      "[u'BlackScholes', u'batchCUBLAS', 1]\n",
      "[u'boxFilterNPP', u'conjugateGradient', 1]\n",
      "[u'boxFilterNPP', u'histogram', 1]\n",
      "[u'boxFilterNPP', u'FDTD3d', 1]\n",
      "[u'boxFilterNPP', u'convolutionSeparable', 1]\n",
      "[u'boxFilterNPP', u'nvgraphPagerank', 1]\n",
      "[u'boxFilterNPP', u'batchCUBLAS', 1]\n",
      "[u'boxFilterNPP', u'dct8x8', 1]\n",
      "[u'boxFilterNPP', u'simpleCUFFTcallback', 1]\n",
      "[u'boxFilterNPP', u'BlackScholes', 1]\n",
      "[u'convolutionSeparable', u'BlackScholes', 1]\n",
      "[u'convolutionSeparable', u'boxFilterNPP', 1]\n",
      "[u'convolutionSeparable', u'nvgraphPagerank', 1]\n",
      "[u'convolutionSeparable', u'conjugateGradient', 1]\n",
      "[u'convolutionSeparable', u'histogram', 1]\n",
      "[u'convolutionSeparable', u'simpleCUFFTcallback', 1]\n",
      "[u'convolutionSeparable', u'dct8x8', 1]\n",
      "[u'convolutionSeparable', u'batchCUBLAS', 1]\n",
      "[u'convolutionSeparable', u'FDTD3d', 1]\n",
      "[u'histogram', u'BlackScholes', 1]\n",
      "[u'histogram', u'nvgraphPagerank', 1]\n",
      "[u'histogram', u'dct8x8', 1]\n",
      "[u'histogram', u'simpleCUFFTcallback', 1]\n",
      "[u'histogram', u'boxFilterNPP', 1]\n",
      "[u'histogram', u'conjugateGradient', 1]\n",
      "[u'histogram', u'FDTD3d', 1]\n",
      "[u'histogram', u'batchCUBLAS', 1]\n",
      "[u'histogram', u'convolutionSeparable', 1]\n",
      "[u'batchCUBLAS', u'FDTD3d', 1]\n",
      "[u'batchCUBLAS', u'boxFilterNPP', 1]\n",
      "[u'batchCUBLAS', u'nvgraphPagerank', 1]\n",
      "[u'batchCUBLAS', u'dct8x8', 1]\n",
      "[u'batchCUBLAS', u'BlackScholes', 1]\n",
      "[u'batchCUBLAS', u'histogram', 1]\n",
      "[u'batchCUBLAS', u'convolutionSeparable', 1]\n",
      "[u'batchCUBLAS', u'conjugateGradient', 1]\n",
      "[u'batchCUBLAS', u'simpleCUFFTcallback', 1]\n",
      "[u'nvgraphPagerank', u'conjugateGradient', 1]\n",
      "[u'nvgraphPagerank', u'FDTD3d', 1]\n",
      "[u'nvgraphPagerank', u'batchCUBLAS', 1]\n",
      "[u'nvgraphPagerank', u'convolutionSeparable', 1]\n",
      "[u'nvgraphPagerank', u'histogram', 1]\n",
      "[u'nvgraphPagerank', u'boxFilterNPP', 1]\n",
      "[u'nvgraphPagerank', u'simpleCUFFTcallback', 1]\n",
      "[u'nvgraphPagerank', u'BlackScholes', 1]\n",
      "[u'nvgraphPagerank', u'dct8x8', 1]\n",
      "[u'conjugateGradient', u'convolutionSeparable', 1]\n",
      "[u'conjugateGradient', u'nvgraphPagerank', 1]\n",
      "[u'conjugateGradient', u'BlackScholes', 1]\n",
      "[u'conjugateGradient', u'boxFilterNPP', 1]\n",
      "[u'conjugateGradient', u'batchCUBLAS', 1]\n",
      "[u'conjugateGradient', u'FDTD3d', 1]\n",
      "[u'conjugateGradient', u'simpleCUFFTcallback', 1]\n",
      "[u'conjugateGradient', u'histogram', 1]\n",
      "[u'conjugateGradient', u'dct8x8', 1]\n",
      "[u'simpleCUFFTcallback', u'histogram', 1]\n",
      "[u'simpleCUFFTcallback', u'FDTD3d', 1]\n",
      "[u'simpleCUFFTcallback', u'conjugateGradient', 1]\n",
      "[u'simpleCUFFTcallback', u'batchCUBLAS', 1]\n",
      "[u'simpleCUFFTcallback', u'boxFilterNPP', 1]\n",
      "[u'simpleCUFFTcallback', u'convolutionSeparable', 1]\n",
      "[u'simpleCUFFTcallback', u'nvgraphPagerank', 1]\n",
      "[u'simpleCUFFTcallback', u'dct8x8', 1]\n",
      "[u'simpleCUFFTcallback', u'BlackScholes', 1]\n",
      "[u'dct8x8', u'simpleCUFFTcallback', 1]\n",
      "[u'dct8x8', u'FDTD3d', 1]\n",
      "[u'dct8x8', u'conjugateGradient', 1]\n",
      "[u'dct8x8', u'batchCUBLAS', 1]\n",
      "[u'dct8x8', u'BlackScholes', 1]\n",
      "[u'dct8x8', u'nvgraphPagerank', 1]\n",
      "[u'dct8x8', u'convolutionSeparable', 1]\n",
      "[u'dct8x8', u'boxFilterNPP', 1]\n",
      "[u'dct8x8', u'histogram', 1]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Measure the performance for the combination of running two applications concurrently the same gpu\n",
    "#\n",
    "import json\n",
    "\n",
    "#\n",
    "# 13 cuda apps\n",
    "#\n",
    "with open('../00_featSel/contention_tests/combo_truth.json') as json_file:  \n",
    "    combo_list = json.load(json_file)\n",
    "    \n",
    "unique_appName = set()\n",
    "for eachCombo in combo_list:\n",
    "    print eachCombo\n",
    "    unique_appName.add(eachCombo[0])\n",
    "\n",
    "#\n",
    "# 10 extra cuda apps\n",
    "#\n",
    "with open('./combo_truth_testApps.json') as json_file:  \n",
    "    combo_list_extra = json.load(json_file)\n",
    "    \n",
    "for eachCombo in combo_list_extra:\n",
    "    print eachCombo\n",
    "    unique_appName.add(eachCombo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxFilterNPP\n",
      "scan\n",
      "[Warning] convolutionFFT2D is not in the same app name.\n",
      "convolutionSeparable\n",
      "mergeSort\n",
      "[Warning] MC_SingleAsianOptionP is not in the same app name.\n",
      "SobolQRNG\n",
      "matrixMul\n",
      "BlackScholes\n",
      "reduction\n",
      "binopt\n",
      "sortingNetworks\n",
      "FDTD3d\n",
      "[Warning] simpleCUFFT_callback is not in the same app name.\n",
      "transpose\n",
      "histogram\n",
      "quasirandomGenerator\n",
      "conjugateGradient\n",
      "dct8x8\n",
      "[Warning] nvgraph_Pagerank is not in the same app name.\n",
      "batchCUBLAS\n",
      "interval\n",
      "radixSortThrust\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# print out unique apps\n",
    "#\n",
    "for key, value in app_rowid_dict.iteritems():\n",
    "    if key in unique_appName:\n",
    "        print key\n",
    "    else:\n",
    "        print(\"[Warning] {} is not in the same app name.\".format(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### buid the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = None\n",
    "y_label = np.zeros(len(combo_list) + len(combo_list_extra), dtype=np.int32)\n",
    "\n",
    "#\n",
    "# 13 cuda apps\n",
    "#\n",
    "count = 0\n",
    "for eachCombo in combo_list:\n",
    "    [app1, app2, goodCombo]= eachCombo\n",
    "    #print app1, app2, goodCombo\n",
    "    \n",
    "    if app1 == \"MCSingleAsianOptionP\": app1 = \"MC_SingleAsianOptionP\"\n",
    "    if app1 == \"convfft2d\":            app1 = \"convolutionFFT2D\"\n",
    "    \n",
    "    if app2 == \"MCSingleAsianOptionP\": app2 = \"MC_SingleAsianOptionP\"\n",
    "    if app2 == \"convfft2d\":            app2 = \"convolutionFFT2D\"\n",
    "        \n",
    "    row1 = app_rowid_dict[app1]\n",
    "    row2 = app_rowid_dict[app2]\n",
    "    \n",
    "    arr1 = app_feat_array[row1] # read feature vector for the app\n",
    "    arr2 = app_feat_array[row2]\n",
    "\n",
    "    currentCombo = np.append(arr1, arr2)\n",
    "    \n",
    "    if count == 0:\n",
    "        X_input = currentCombo\n",
    "    else:\n",
    "        X_input = np.vstack((X_input, currentCombo))\n",
    "        \n",
    "    y_label[count] = int(goodCombo)\n",
    "    \n",
    "    count = count + 1\n",
    "\n",
    "\n",
    "#\n",
    "# extra 10 apps\n",
    "#\n",
    "for eachCombo in combo_list_extra:\n",
    "    [app1, app2, goodCombo]= eachCombo\n",
    "    #print app1, app2, goodCombo\n",
    "    \n",
    "    if app1 == \"nvgraphPagerank\":                app1 = \"nvgraph_Pagerank\"\n",
    "    if app1 == \"simpleCUFFTcallback\":            app1 = \"simpleCUFFT_callback\"\n",
    "    \n",
    "    if app2 == \"nvgraphPagerank\":                app2 = \"nvgraph_Pagerank\"\n",
    "    if app2 == \"simpleCUFFTcallback\":            app2 = \"simpleCUFFT_callback\"\n",
    "        \n",
    "    row1 = app_rowid_dict[app1]\n",
    "    row2 = app_rowid_dict[app2]\n",
    "    \n",
    "    arr1 = app_feat_array[row1]\n",
    "    arr2 = app_feat_array[row2]\n",
    "\n",
    "    currentCombo = np.append(arr1, arr2)\n",
    "    \n",
    "    X_input = np.vstack((X_input, currentCombo))\n",
    "        \n",
    "    y_label[count] = int(goodCombo)\n",
    "    \n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check X_input and y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 90)\n",
      "(246,)\n"
     ]
    }
   ],
   "source": [
    "print X_input.shape\n",
    "print y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print combo_list[0]\n",
    "# print y_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 90)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                              \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "n_inputs = 90\n",
    "n_hidden = 500    # tunning the layer size here!!!\n",
    "n_outputs = 2    # good (1) or bad (0)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # avoid repeating the same parameters over and over again\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "            training=training, momentum=batch_norm_momentum)\n",
    "    \n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer=he_init) # activeFunc after BN\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden, name=\"hidden1\")\n",
    "    bn1= my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = my_dense_layer(bn1_act, n_hidden, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    hidden3 = my_dense_layer(bn2_act, n_hidden, name=\"hidden3\")\n",
    "    bn3 = my_batch_norm_layer(hidden3)\n",
    "    bn3_act = tf.nn.elu(bn3)\n",
    "\n",
    "    hidden4 = my_dense_layer(bn3_act, n_hidden, name=\"hidden4\")\n",
    "    bn4_act = tf.nn.elu(my_batch_norm_layer(hidden4))\n",
    "    \n",
    "    hidden5 = my_dense_layer(bn4_act, n_hidden, name=\"hidden5\")\n",
    "    bn5_act = tf.nn.elu(my_batch_norm_layer(hidden5))\n",
    "    \n",
    "    hidden6 = my_dense_layer(bn5_act, n_hidden, name=\"hidden6\")\n",
    "    bn6_act = tf.nn.elu(my_batch_norm_layer(hidden6))\n",
    "    \n",
    "    hidden7 = my_dense_layer(bn6_act, n_hidden, name=\"hidden7\")\n",
    "    bn7_act = tf.nn.elu(my_batch_norm_layer(hidden7))\n",
    "    \n",
    "    hidden8 = my_dense_layer(bn7_act, n_hidden, name=\"hidden8\")\n",
    "    bn8_act = tf.nn.elu(my_batch_norm_layer(hidden8))\n",
    "    \n",
    "    hidden9 = my_dense_layer(bn8_act, n_hidden, name=\"hidden9\")\n",
    "    bn9_act = tf.nn.elu(my_batch_norm_layer(hidden9))\n",
    "    \n",
    "    hidden10 = my_dense_layer(bn9_act, n_hidden, name=\"hidden10\")\n",
    "    bn10_act = tf.nn.elu(my_batch_norm_layer(hidden10))\n",
    "    \n",
    "    # add extra 10 layer\n",
    "    hidden11 = my_dense_layer(bn10_act, n_hidden, name=\"hidden11\")\n",
    "    bn11_act = tf.nn.elu(my_batch_norm_layer(hidden11))\n",
    "    \n",
    "    hidden12 = my_dense_layer(bn11_act, n_hidden, name=\"hidden12\")\n",
    "    bn12_act = tf.nn.elu(my_batch_norm_layer(hidden12))\n",
    "    \n",
    "    hidden13 = my_dense_layer(bn12_act, n_hidden, name=\"hidden13\")\n",
    "    bn13_act = tf.nn.elu(my_batch_norm_layer(hidden13))\n",
    "    \n",
    "    hidden14 = my_dense_layer(bn13_act, n_hidden, name=\"hidden14\")\n",
    "    bn14_act = tf.nn.elu(my_batch_norm_layer(hidden14))\n",
    "    \n",
    "    hidden15 = my_dense_layer(bn14_act, n_hidden, name=\"hidden15\")\n",
    "    bn15_act = tf.nn.elu(my_batch_norm_layer(hidden15))\n",
    "    \n",
    "    hidden16 = my_dense_layer(bn15_act, n_hidden, name=\"hidden16\")\n",
    "    bn16_act = tf.nn.elu(my_batch_norm_layer(hidden16))\n",
    "    \n",
    "    hidden17 = my_dense_layer(bn16_act, n_hidden, name=\"hidden17\")\n",
    "    bn17_act = tf.nn.elu(my_batch_norm_layer(hidden17))\n",
    "    \n",
    "    hidden18 = my_dense_layer(bn17_act, n_hidden, name=\"hidden18\")\n",
    "    bn18_act = tf.nn.elu(my_batch_norm_layer(hidden18))\n",
    "    \n",
    "    hidden19 = my_dense_layer(bn18_act, n_hidden, name=\"hidden19\")\n",
    "    bn19_act = tf.nn.elu(my_batch_norm_layer(hidden19))\n",
    "    \n",
    "    hidden20 = my_dense_layer(bn19_act, n_hidden, name=\"hidden20\")\n",
    "    bn20_act = tf.nn.elu(my_batch_norm_layer(hidden20))\n",
    "    \n",
    "    \n",
    "    # add extra 10 layer\n",
    "    hidden21 = my_dense_layer(bn20_act, n_hidden, name=\"hidden21\")\n",
    "    bn21_act = tf.nn.elu(my_batch_norm_layer(hidden21))\n",
    "    \n",
    "    hidden22 = my_dense_layer(bn21_act, n_hidden, name=\"hidden22\")\n",
    "    bn22_act = tf.nn.elu(my_batch_norm_layer(hidden22))\n",
    "    \n",
    "    hidden23 = my_dense_layer(bn22_act, n_hidden, name=\"hidden23\")\n",
    "    bn23_act = tf.nn.elu(my_batch_norm_layer(hidden23))\n",
    "    \n",
    "    hidden24 = my_dense_layer(bn23_act, n_hidden, name=\"hidden24\")\n",
    "    bn24_act = tf.nn.elu(my_batch_norm_layer(hidden24))\n",
    "    \n",
    "    hidden25 = my_dense_layer(bn24_act, n_hidden, name=\"hidden25\")\n",
    "    bn25_act = tf.nn.elu(my_batch_norm_layer(hidden25))\n",
    "    \n",
    "    hidden26 = my_dense_layer(bn25_act, n_hidden, name=\"hidden26\")\n",
    "    bn26_act = tf.nn.elu(my_batch_norm_layer(hidden26))\n",
    "    \n",
    "    hidden27 = my_dense_layer(bn26_act, n_hidden, name=\"hidden27\")\n",
    "    bn27_act = tf.nn.elu(my_batch_norm_layer(hidden27))\n",
    "    \n",
    "    hidden28 = my_dense_layer(bn27_act, n_hidden, name=\"hidden28\")\n",
    "    bn28_act = tf.nn.elu(my_batch_norm_layer(hidden28))\n",
    "    \n",
    "    hidden29 = my_dense_layer(bn28_act, n_hidden, name=\"hidden29\")\n",
    "    bn29_act = tf.nn.elu(my_batch_norm_layer(hidden29))\n",
    "    \n",
    "    hidden30 = my_dense_layer(bn29_act, n_hidden, name=\"hidden30\")\n",
    "    bn30_act = tf.nn.elu(my_batch_norm_layer(hidden30))\n",
    "    \n",
    "    logits_before_bn = my_dense_layer(bn30_act, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples = 246 , n_batches = 2\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 96\n",
    "\n",
    "total_samples = X_input.shape[0]\n",
    "n_batches = int(np.ceil(total_samples / batch_size))\n",
    "print 'total_samples = %d , n_batches = %d' % (total_samples, n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
      " 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def next_batch(X_input, y_label, batch_size):\n",
    "    samples = X_input.shape[0]\n",
    "    idx = np.arange(0 , samples)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    \n",
    "    input_shuffle = [X_input[i,:] for i in idx]\n",
    "    label_shuffle = [y_label[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(input_shuffle), np.asarray(label_shuffle)\n",
    "    \n",
    "### test\n",
    "X_batch, y_batch = next_batch(X_input, y_label, batch_size)\n",
    "print y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# log for tensorboard\n",
    "#\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Batch accuracy:', 0.48958334)\n",
      "(5, 'Batch accuracy:', 0.67708331)\n",
      "(10, 'Batch accuracy:', 0.79166669)\n",
      "(15, 'Batch accuracy:', 0.80208331)\n",
      "(20, 'Batch accuracy:', 0.77083331)\n",
      "(25, 'Batch accuracy:', 0.8125)\n",
      "(30, 'Batch accuracy:', 0.79166669)\n",
      "(35, 'Batch accuracy:', 0.84375)\n",
      "(40, 'Batch accuracy:', 0.79166669)\n",
      "(45, 'Batch accuracy:', 0.78125)\n",
      "(50, 'Batch accuracy:', 0.80208331)\n",
      "(55, 'Batch accuracy:', 0.82291669)\n",
      "(60, 'Batch accuracy:', 0.86458331)\n",
      "(65, 'Batch accuracy:', 0.84375)\n",
      "(70, 'Batch accuracy:', 0.83333331)\n",
      "(75, 'Batch accuracy:', 0.83333331)\n",
      "(80, 'Batch accuracy:', 0.88541669)\n",
      "(85, 'Batch accuracy:', 0.88541669)\n",
      "(90, 'Batch accuracy:', 0.78125)\n",
      "(95, 'Batch accuracy:', 0.79166669)\n",
      "(100, 'Batch accuracy:', 0.83333331)\n",
      "(105, 'Batch accuracy:', 0.8125)\n",
      "(110, 'Batch accuracy:', 0.80208331)\n",
      "(115, 'Batch accuracy:', 0.79166669)\n",
      "(120, 'Batch accuracy:', 0.83333331)\n",
      "(125, 'Batch accuracy:', 0.88541669)\n",
      "(130, 'Batch accuracy:', 0.83333331)\n",
      "(135, 'Batch accuracy:', 0.79166669)\n",
      "(140, 'Batch accuracy:', 0.80208331)\n",
      "(145, 'Batch accuracy:', 0.8125)\n",
      "(150, 'Batch accuracy:', 0.78125)\n",
      "(155, 'Batch accuracy:', 0.83333331)\n",
      "(160, 'Batch accuracy:', 0.75)\n",
      "(165, 'Batch accuracy:', 0.84375)\n",
      "(170, 'Batch accuracy:', 0.8125)\n",
      "(175, 'Batch accuracy:', 0.86458331)\n",
      "(180, 'Batch accuracy:', 0.84375)\n",
      "(185, 'Batch accuracy:', 0.86458331)\n",
      "(190, 'Batch accuracy:', 0.8125)\n",
      "(195, 'Batch accuracy:', 0.875)\n",
      "(200, 'Batch accuracy:', 0.8125)\n",
      "(205, 'Batch accuracy:', 0.84375)\n",
      "(210, 'Batch accuracy:', 0.85416669)\n",
      "(215, 'Batch accuracy:', 0.82291669)\n",
      "(220, 'Batch accuracy:', 0.82291669)\n",
      "(225, 'Batch accuracy:', 0.88541669)\n",
      "(230, 'Batch accuracy:', 0.8125)\n",
      "(235, 'Batch accuracy:', 0.83333331)\n",
      "(240, 'Batch accuracy:', 0.83333331)\n",
      "(245, 'Batch accuracy:', 0.875)\n",
      "(250, 'Batch accuracy:', 0.79166669)\n",
      "(255, 'Batch accuracy:', 0.78125)\n",
      "(260, 'Batch accuracy:', 0.8125)\n",
      "(265, 'Batch accuracy:', 0.79166669)\n",
      "(270, 'Batch accuracy:', 0.84375)\n",
      "(275, 'Batch accuracy:', 0.76041669)\n",
      "(280, 'Batch accuracy:', 0.84375)\n",
      "(285, 'Batch accuracy:', 0.82291669)\n",
      "(290, 'Batch accuracy:', 0.82291669)\n",
      "(295, 'Batch accuracy:', 0.83333331)\n",
      "(300, 'Batch accuracy:', 0.84375)\n",
      "(305, 'Batch accuracy:', 0.8125)\n",
      "(310, 'Batch accuracy:', 0.83333331)\n",
      "(315, 'Batch accuracy:', 0.86458331)\n",
      "(320, 'Batch accuracy:', 0.77083331)\n",
      "(325, 'Batch accuracy:', 0.92708331)\n",
      "=> Found a good model at accuracy : 0.927083313465   (Saving the model ... )\n",
      "(330, 'Batch accuracy:', 0.80208331)\n",
      "(335, 'Batch accuracy:', 0.90625)\n",
      "(340, 'Batch accuracy:', 0.84375)\n",
      "(345, 'Batch accuracy:', 0.79166669)\n",
      "(350, 'Batch accuracy:', 0.85416669)\n",
      "(355, 'Batch accuracy:', 0.8125)\n",
      "(360, 'Batch accuracy:', 0.82291669)\n",
      "(365, 'Batch accuracy:', 0.80208331)\n",
      "(370, 'Batch accuracy:', 0.80208331)\n",
      "(375, 'Batch accuracy:', 0.86458331)\n",
      "(380, 'Batch accuracy:', 0.82291669)\n",
      "(385, 'Batch accuracy:', 0.77083331)\n",
      "(390, 'Batch accuracy:', 0.80208331)\n",
      "(395, 'Batch accuracy:', 0.82291669)\n",
      "(400, 'Batch accuracy:', 0.78125)\n",
      "(405, 'Batch accuracy:', 0.85416669)\n",
      "(410, 'Batch accuracy:', 0.84375)\n",
      "(415, 'Batch accuracy:', 0.875)\n",
      "(420, 'Batch accuracy:', 0.80208331)\n",
      "(425, 'Batch accuracy:', 0.82291669)\n",
      "(430, 'Batch accuracy:', 0.78125)\n",
      "(435, 'Batch accuracy:', 0.84375)\n",
      "(440, 'Batch accuracy:', 0.79166669)\n",
      "(445, 'Batch accuracy:', 0.76041669)\n",
      "(450, 'Batch accuracy:', 0.85416669)\n",
      "(455, 'Batch accuracy:', 0.85416669)\n",
      "(460, 'Batch accuracy:', 0.82291669)\n",
      "(465, 'Batch accuracy:', 0.79166669)\n",
      "(470, 'Batch accuracy:', 0.82291669)\n",
      "(475, 'Batch accuracy:', 0.8125)\n",
      "(480, 'Batch accuracy:', 0.77083331)\n",
      "(485, 'Batch accuracy:', 0.78125)\n",
      "(490, 'Batch accuracy:', 0.82291669)\n",
      "(495, 'Batch accuracy:', 0.85416669)\n",
      "(500, 'Batch accuracy:', 0.76041669)\n",
      "(505, 'Batch accuracy:', 0.84375)\n",
      "(510, 'Batch accuracy:', 0.80208331)\n",
      "(515, 'Batch accuracy:', 0.80208331)\n",
      "(520, 'Batch accuracy:', 0.80208331)\n",
      "(525, 'Batch accuracy:', 0.80208331)\n",
      "(530, 'Batch accuracy:', 0.83333331)\n",
      "(535, 'Batch accuracy:', 0.80208331)\n",
      "(540, 'Batch accuracy:', 0.86458331)\n",
      "(545, 'Batch accuracy:', 0.8125)\n",
      "(550, 'Batch accuracy:', 0.8125)\n",
      "(555, 'Batch accuracy:', 0.78125)\n",
      "(560, 'Batch accuracy:', 0.84375)\n",
      "(565, 'Batch accuracy:', 0.80208331)\n",
      "(570, 'Batch accuracy:', 0.76041669)\n",
      "(575, 'Batch accuracy:', 0.84375)\n",
      "(580, 'Batch accuracy:', 0.80208331)\n",
      "(585, 'Batch accuracy:', 0.82291669)\n",
      "(590, 'Batch accuracy:', 0.8125)\n",
      "(595, 'Batch accuracy:', 0.79166669)\n",
      "(600, 'Batch accuracy:', 0.85416669)\n",
      "(605, 'Batch accuracy:', 0.79166669)\n",
      "(610, 'Batch accuracy:', 0.79166669)\n",
      "(615, 'Batch accuracy:', 0.77083331)\n",
      "(620, 'Batch accuracy:', 0.73958331)\n",
      "(625, 'Batch accuracy:', 0.85416669)\n",
      "(630, 'Batch accuracy:', 0.80208331)\n",
      "(635, 'Batch accuracy:', 0.79166669)\n",
      "(640, 'Batch accuracy:', 0.82291669)\n",
      "(645, 'Batch accuracy:', 0.77083331)\n",
      "(650, 'Batch accuracy:', 0.82291669)\n",
      "(655, 'Batch accuracy:', 0.80208331)\n",
      "(660, 'Batch accuracy:', 0.76041669)\n",
      "(665, 'Batch accuracy:', 0.82291669)\n",
      "(670, 'Batch accuracy:', 0.82291669)\n",
      "(675, 'Batch accuracy:', 0.76041669)\n",
      "(680, 'Batch accuracy:', 0.80208331)\n",
      "(685, 'Batch accuracy:', 0.79166669)\n",
      "(690, 'Batch accuracy:', 0.76041669)\n",
      "(695, 'Batch accuracy:', 0.80208331)\n",
      "(700, 'Batch accuracy:', 0.82291669)\n",
      "(705, 'Batch accuracy:', 0.8125)\n",
      "(710, 'Batch accuracy:', 0.82291669)\n",
      "(715, 'Batch accuracy:', 0.78125)\n",
      "(720, 'Batch accuracy:', 0.78125)\n",
      "(725, 'Batch accuracy:', 0.875)\n",
      "(730, 'Batch accuracy:', 0.84375)\n",
      "(735, 'Batch accuracy:', 0.83333331)\n",
      "(740, 'Batch accuracy:', 0.80208331)\n",
      "(745, 'Batch accuracy:', 0.80208331)\n",
      "(750, 'Batch accuracy:', 0.91666669)\n",
      "(755, 'Batch accuracy:', 0.77083331)\n",
      "(760, 'Batch accuracy:', 0.76041669)\n",
      "(765, 'Batch accuracy:', 0.77083331)\n",
      "(770, 'Batch accuracy:', 0.84375)\n",
      "(775, 'Batch accuracy:', 0.80208331)\n",
      "(780, 'Batch accuracy:', 0.80208331)\n",
      "(785, 'Batch accuracy:', 0.76041669)\n",
      "(790, 'Batch accuracy:', 0.79166669)\n",
      "(795, 'Batch accuracy:', 0.82291669)\n",
      "(800, 'Batch accuracy:', 0.80208331)\n",
      "(805, 'Batch accuracy:', 0.80208331)\n",
      "(810, 'Batch accuracy:', 0.84375)\n",
      "(815, 'Batch accuracy:', 0.83333331)\n",
      "(820, 'Batch accuracy:', 0.82291669)\n",
      "(825, 'Batch accuracy:', 0.79166669)\n",
      "(830, 'Batch accuracy:', 0.80208331)\n",
      "(835, 'Batch accuracy:', 0.83333331)\n",
      "(840, 'Batch accuracy:', 0.78125)\n",
      "(845, 'Batch accuracy:', 0.82291669)\n",
      "(850, 'Batch accuracy:', 0.85416669)\n",
      "(855, 'Batch accuracy:', 0.83333331)\n",
      "(860, 'Batch accuracy:', 0.83333331)\n",
      "(865, 'Batch accuracy:', 0.86458331)\n",
      "(870, 'Batch accuracy:', 0.82291669)\n",
      "(875, 'Batch accuracy:', 0.83333331)\n",
      "(880, 'Batch accuracy:', 0.83333331)\n",
      "(885, 'Batch accuracy:', 0.79166669)\n",
      "(890, 'Batch accuracy:', 0.78125)\n",
      "(895, 'Batch accuracy:', 0.83333331)\n",
      "(900, 'Batch accuracy:', 0.82291669)\n",
      "(905, 'Batch accuracy:', 0.79166669)\n",
      "(910, 'Batch accuracy:', 0.8125)\n",
      "(915, 'Batch accuracy:', 0.85416669)\n",
      "(920, 'Batch accuracy:', 0.73958331)\n",
      "(925, 'Batch accuracy:', 0.83333331)\n",
      "(930, 'Batch accuracy:', 0.85416669)\n",
      "(935, 'Batch accuracy:', 0.79166669)\n",
      "(940, 'Batch accuracy:', 0.75)\n",
      "(945, 'Batch accuracy:', 0.80208331)\n",
      "(950, 'Batch accuracy:', 0.82291669)\n",
      "(955, 'Batch accuracy:', 0.84375)\n",
      "(960, 'Batch accuracy:', 0.79166669)\n",
      "(965, 'Batch accuracy:', 0.82291669)\n",
      "(970, 'Batch accuracy:', 0.80208331)\n",
      "(975, 'Batch accuracy:', 0.8125)\n",
      "(980, 'Batch accuracy:', 0.83333331)\n",
      "(985, 'Batch accuracy:', 0.8125)\n",
      "(990, 'Batch accuracy:', 0.83333331)\n",
      "(995, 'Batch accuracy:', 0.84375)\n",
      "Best Accuracy Achieved : 0.927083313465\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "BestAcc = 0.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches): # go through all the batches\n",
    "            # 1)  generate batch\n",
    "            X_batch, y_batch = next_batch(X_input, y_label, batch_size)\n",
    "            \n",
    "            # 2) save the loss (at the 5th batch)\n",
    "            if iteration % 2 == 0:\n",
    "                summary_str = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})  # save the loss func values\n",
    "                step = epoch * n_batches + iteration\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "                \n",
    "            # 3) run the training\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "            \n",
    "        # check the accuracy every 5 epoches\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch,\n",
    "                                                 y: y_batch})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train)\n",
    "            \n",
    "            if acc_train > BestAcc:\n",
    "                BestAcc = acc_train\n",
    "                \n",
    "            if acc_train > 0.92:\n",
    "                # save the model\n",
    "                print('=> Found a good model at accuracy : {}   (Saving the model ... )'.format(acc_train))\n",
    "                save_path = saver.save(sess, \"./dinn_final.ckpt\")\n",
    "\n",
    "                \n",
    "print(\"Best Accuracy Achieved : {}\".format(BestAcc))\n",
    "\n",
    "#\n",
    "# end logging\n",
    "# \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./dinn_final.ckpt\n",
      "[-1.46900022  1.37378049]\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./dinn_final.ckpt\")\n",
    "    logits_pred, correct_pred = sess.run([logits, correct], feed_dict={X: X_input, y: y_label})\n",
    "    print logits_pred[0]\n",
    "    print y_label[0]\n",
    "    print correct_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = X_input[0,:]\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # b = np.transpose(a)\n",
    "# b = a.T\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = np.reshape(a, (1,90))\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"./model_final_30hdly_500lysize_bah128.ckpt\")\n",
    "#     logits_pred = sess.run([logits, correct], feed_dict={X: b, y: y_label[0]})\n",
    "#     print logits_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
